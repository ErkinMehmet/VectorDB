{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2103f418-85bf-49e0-a165-cc6594bbda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install chromadb openai langchain tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095c3bff-e503-4f71-ab74-396717aebe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: chromadb\n",
      "Version: 1.3.4\n",
      "Summary: Chroma.\n",
      "Home-page: https://github.com/chroma-core/chroma\n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: c:\\users\\hli\\appdata\\roaming\\python\\python310\\site-packages\n",
      "Requires: bcrypt, build, grpcio, httpx, importlib-resources, jsonschema, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-sdk, orjson, overrides, posthog, pybase64, pydantic, pypika, pyyaml, rich, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f6cb2-083c-4a8c-9660-f8e5d015affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://www.dropbox.com/s/vs6ocyvpzzncvwh/new_articles.zip\n",
    "!unzip -q new_articles.zip -d new_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f79f30-21e6-4032-9929-ef98dd4d80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environement\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader,TextLoader\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# load data\n",
    "loader=DirectoryLoader(\"/content/new_articles/\",glob=\"./*.txt\",loader_cls=TextLoader) # to load tons of .txt files\n",
    "document=loader.load()\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "text = text_splitter.split_documents(document)\n",
    "\n",
    "# create db\n",
    "from langchain import embeddings\n",
    "persist_directory='db'\n",
    "embedding=OpenAIEmbeddings()\n",
    "verctordb=Chroma.from_documents(documents=text,embedding=embedding,persist_directory=persist_directory)\n",
    "# make the db persist to disk\n",
    "vectordb.persist()\n",
    "verctordb=None()\n",
    "# now we can load the persistent database from disk and use it as normal\n",
    "vectordb=Chroma(persist_directory=persist_directory,embedding_function=embedding)\n",
    "\n",
    "# make a retriever\n",
    "query=\"How much money did Microsoft raise?\"\n",
    "retriever=vectordb.as_retriever()\n",
    "docs=retriever.get_relevant_documents(query)\n",
    "len(docs)\n",
    "retriever.search_kwargs\n",
    "retriever.search_type\n",
    "\n",
    "# make a chain\n",
    "llm=OpenAI()\n",
    "# create a chain to answer the question\n",
    "qa_chain=RetrivalQA.from_chain_type(llm=OpenAI(),\n",
    "                                    chain_type=\"stuff\",\n",
    "                                    retriever=retriever,\n",
    "                                    return_source_documents=True)\n",
    "# cite sources\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    for source in llm_response['source_documents']:\n",
    "        print(source.metadata['source'])\n",
    "\n",
    "llm_response=qa_chain(query)\n",
    "process_llm_response(llm_response)\n",
    "\n",
    "# deleting the db\n",
    "!zip -r db.zip ./db\n",
    "vectordb.delete_collection()\n",
    "vectordb.persist()\n",
    "\n",
    "# delete the directory\n",
    "!rm -rf db/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512f4d0-e8a5-4961-b049-e55de0f4f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone\n",
    "!pip install langchain\n",
    "!pip install pinecone-client\n",
    "!pip install pypdf\n",
    "from langchain.document_loader import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrieveQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# load file\n",
    "!mkdir pdfs\n",
    "!gdown 1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE -O pdfs/yolov7paper.pdf\n",
    "!gdown 1vILwiv6nS2wI3chxNabMgry3qnV67TxM -O pdfs/rachelgreecv.pdf\n",
    "\n",
    "# extract the text from the pdfs\n",
    "loader=PyPDFDirectoryLoader(\"pdfs\")\n",
    "data=loader.load()\n",
    "# split the texts into chunks\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=00,chunk_overlap=20)\n",
    "text_chunks=text_splitter.split_documents(data)\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "embeddings=OpenAIEmbeddings()\n",
    "result=embeddings.embed_query(\"how are you?\")\n",
    "\n",
    "# init pinecone\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', 're45re12r-et45e-4965-9035-e09c00ad18a5')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')\n",
    "\n",
    "import pinecone\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"test\" # put in the name of your pinecone index here\n",
    "\n",
    "# create embeddings for each of the text chunk\n",
    "docsearch=Pinecone.from_texts([t.page_content for t in text_chunks],embeddings,index_name=index_name)\n",
    "# if you already have an index, we can load it like this\n",
    "docsearch=Pinecone.from_existing_index(index_name,embeddings)\n",
    "# similarity search\n",
    "query=\"YOLOv7 outperforms which models\"\n",
    "docs=docsearch.similarity_search(query,k=3)\n",
    "docs\n",
    "# createa a llm model wrapper\n",
    "llm=OpenAI()\n",
    "qa=RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\",retriever=docsearch.as_retriever())\n",
    "qa.run(query)\n",
    "\n",
    "import sys\n",
    "while True:\n",
    "    user_input=input(\"input:\")\n",
    "    if user_input=\"exit\":\n",
    "        sys.exist()\n",
    "    if user_input=\"\":\n",
    "        continue\n",
    "    result=qa({'query':user_input})\n",
    "    print(f\"answer:{result['result']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411e644-6840-44d4-ae0d-f7db642426d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weaviate\n",
    "!pip install weaviate-client\n",
    "!pip install langchain\n",
    "!pip install openai\n",
    "OPENAI_API_KEY = \"\"\n",
    "WEAVIATE_API_KEY = \"\"\n",
    "WEAVIATE_CLUSTER = \"https://test-njh6t5hm.weaviate.network\"\n",
    "!mkdir data\n",
    "!pip install unstructured\n",
    "!pip install \"unstructured[pdf]\"\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./data\",glob = \"**/*.pdf\")\n",
    "data = loader.load()\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(data)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key= OPENAI_API_KEY)\n",
    "\n",
    "import weaviate\n",
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "#Connect to weaviate Cluster\n",
    "auth_config = weaviate.auth.AuthApiKey(api_key = WEAVIATE_API_KEY)\n",
    "WEAVIATE_URL = WEAVIATE_CLUSTER\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = WEAVIATE_URL,\n",
    "    additional_headers = {\"X-OpenAI-Api-key\": OPENAI_API_KEY},\n",
    "    auth_client_secret = auth_config,\n",
    "    startup_period = 10\n",
    ")\n",
    "client.is_ready()\n",
    "\n",
    "# define input structure\n",
    "client.schema.delete_all()\n",
    "client.schema.get()\n",
    "schema={\n",
    "    \"classes\":\"Chatbot\",\n",
    "    \"description\":\"Document for chatbot\",\n",
    "    \"vectorizer\": \"text2vec-openai\",\n",
    "    \"moduleConfig\": {\"text2vec-openai\":{\"model\":\"ada\",\"type\":\"text\"}},\n",
    "    \"properties\":[\n",
    "        {\n",
    "            \"dataType\":[\"text\"],\n",
    "            \"description\":\"The content of the paragraph\",\n",
    "            \"moduleConfig\":{\n",
    "                \"text1vec-api\":{\n",
    "                    \"skip\":False,\n",
    "                    \"vectorizePropertyName\":False,\n",
    "                }\n",
    "\n",
    "            },\n",
    "            \"name\":\"content\",\n",
    "        },\n",
    "    ],\n",
    "},],}\n",
    "client.schema.create(schema)\n",
    "vectorstore=Weaviate(client,\"Chatbot\",content,attribute=[\"source\"])\n",
    "\n",
    "# load text into the vectorstore\n",
    "text_meta_pair=[(doc.page_content,doc.metadata) for doc in docs]\n",
    "texts,meta=list(zip(*text_meta_pair))\n",
    "vectorstore.add_texts(texts,meta)\n",
    "\n",
    "# similarity mesurement\n",
    "query=\"what is a yolo?\"\n",
    "docs=vectorstore.similarity_search(query,top_k=20)\n",
    "\n",
    "# chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "chain=load_qa_chain(OpenAI(openai_api_key=OPENAI_API_KEY,temperature=0),chain_type=\"stuff\")\n",
    "chain.run(input_documents=docs,question=query) # create answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
